ppo:
    env: DistEnv-v0
    run: PPO
    stop:
        time_total_s:  36000 # 4*60*60
    config:

        env_config:
          max_sequence_skip: 150
          random_start: True
          max_episode_time: 5hours

        # Model
        # If true, use the Generalized Advantage Estimator (GAE)
        use_gae: True
        # GAE(lambda) parameter
        lambda: 0.999
        # Initial coefficient for KL divergence
        kl_coeff: 0.2
        # Override model config
        model:
          # Whether to use LSTM model
          #use_lstm: True
          # Max seq length for LSTM training.
          max_seq_len: 100
          # Lstm cell size
          lstm_cell_size: 64
          # if Fully connected net
          fcnet_hiddens: [64, 64]
          # Free log std
          free_log_std: False
          # Whether to squash the action output to space range
          squash_to_range: True

          custom_preprocessor: mv
          custom_options:
            fast_macd_l: 1200
            slow_macd_l: 2400
            ofi_l: 1000
            mid_l: 1000
        # Which observation filter to apply to the observation
        observation_filter: MeanStdFilter

        # Optimization
        # Number of timesteps collected for each SGD round
        timesteps_per_batch: 100000
        # Whether to rollout "complete_episodes" or "truncate_episodes"
        batch_mode: complete_episodes
        # Number of SGD iterations in each outer loop
        num_sgd_iter: 10
        # Stepsize of SGD
        sgd_stepsize: 0.0005
        # Total SGD batch size across all devices for SGD (multi-gpu only)
        sgd_batchsize: 20480
        # Coefficient of the value function loss
        vf_loss_coeff: 1.0
        # Coefficient of the entropy regularizer
        entropy_coeff: 0.01
        # PPO clip parameter
        clip_param: 0.2
        # Target value for KL divergence
        kl_target: 0.01
        # Use the sync samples optimizer instead of the multi-gpu one
        simple_optimizer: True
        # Resources
        num_workers: 3
        # Number of GPUs to use for SGD
        num_gpus: 0
        # Whether to allocate GPUs for workers (if > 0).
        num_gpus_per_worker: 0
        # Whether to allocate CPUs for workers (if > 0).
        num_cpus_per_worker: 1

    local_dir: ~/PycharmProjects/orderbookrl/logs/distenv
    checkpoint_freq: 10

